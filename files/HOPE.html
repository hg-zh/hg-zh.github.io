<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta http-equiv="Content-Style-Type" content="text/css">
  <title></title>
  <meta name="Generator" content="Cocoa HTML Writer">
  <meta name="CocoaVersion" content="2566">
  <style type="text/css">
    p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 14.0px Helvetica; color: #262626; -webkit-text-stroke: #262626}
    p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 14.0px Helvetica; color: #262626; -webkit-text-stroke: #262626; background-color: #ffffff}
    span.s1 {font-kerning: none; background-color: #ffffff}
    span.s2 {font-kerning: none}
  </style>
</head>
<body>
<p class="p1"><span class="s1">@ARTICLE{10553262,</span></p>
<p class="p1"><span class="s1">author={Zhang, Fan and Zhou, Hang and Hua, Xian-Sheng and Chen, Chong and Luo, Xiao},</span></p>
<p class="p1"><span class="s1">journal={ IEEE Transactions on Pattern Analysis \&amp; Machine Intelligence },</span></p>
<p class="p1"><span class="s1">title={{ Hope: A Hierarchical Perspective for Semi-supervised 2D-3D Cross-Modal Retrieval }},</span></p>
<p class="p1"><span class="s1">year={5555},</span></p>
<p class="p1"><span class="s1">volume={},</span></p>
<p class="p1"><span class="s1">number={01},</span></p>
<p class="p1"><span class="s1">ISSN={1939-3539},</span></p>
<p class="p1"><span class="s1">pages={1-18},</span></p>
<p class="p2"><span class="s2">abstract={ With the emergence of AI generated content, cross-modal retrieval of 2D and 3D data has obtained increasing research attention. In practical applications, massive amounts of 2D and 3D data need expensive annotation, which would make labels scarce. Even worse, complicated heterogeneous relationships between 2D and 3D data make the problem more challenging. In this research, we study the problem of semi-supervised 2D and 3D cross-modal retrieval and provide a novel method named Hierarchical Alignment with Ambiguous Pseudo-labeling (HOPE) for this problem. The core of HOPE is to align two modalities in the common space from a hierarchical perspective. Specifically, HOPE not only enforces each sample to approach its respective modality-invariant anchors from an individual view, but also measures both prototypes and distribution for both modalities for discrepancy reduction from a group view. To handle label scarcity with limited error accumulation, HOPE employs two branches of perturbed networks to generate ambiguous candidates, which guides the cross-branch supervision using a margin-based ranking objective. In addition, we retrieve reliable unlabeled samples for each anchor with curriculum learning and class balance, which are added into labeled datasets to clear ambiguity. Extensive experiments on various benchmark datasets validate the superiority of the proposed HOPE. },</span></p>
<p class="p1"><span class="s1">keywords={Three-dimensional displays;Semantics;Neural networks;Optimization;Semisupervised learning;Feature extraction;Solid modeling},</span></p>
<p class="p1"><span class="s1">doi={10.1109/TPAMI.2024.3412760},</span></p>
<p class="p1"><span class="s1">url = {https://doi.ieeecomputersociety.org/10.1109/TPAMI.2024.3412760},</span></p>
<p class="p1"><span class="s1">publisher={IEEE Computer Society},</span></p>
<p class="p1"><span class="s1">address={Los Alamitos, CA, USA},</span></p>
<p class="p1"><span class="s1">month=jun}</span></p>
</body>
</html>
